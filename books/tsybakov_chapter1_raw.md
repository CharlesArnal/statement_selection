# Chapter 1: Nonparametric Estimators

*From: A. B. Tsybakov, Introduction to Nonparametric Estimation*

---

<!-- Page 11 -->

Nonparametric estimators
1.1 Examples of nonparametric models and problems
1. Estimation of a probability density
Let X1, . . . , Xn be identically distributed real valued random variables whose
common distribution is absolutely continuous with respect to the Lebesgue
measure on R. The density of this distribution, denoted by p, is a function
from R to [0, +âˆ) supposed to be unknown. The problem is to estimate p.
An estimator of p is a function x â†’pn(x) = pn(x, X1, . . . , Xn) measurable
with respect to the observation X = (X1, . . . , Xn). If we know a priori that
p belongs to a parametric family {g(x, Î¸) : Î¸ âˆˆÎ˜}, where g(Â·, Â·) is a given
function, and Î˜ is a subset of Rk with a ï¬xed dimension k independent of
n, then estimation of p is equivalent to estimation of the ï¬nite-dimensional
parameter Î¸. This is a parametric problem of estimation. On the contrary, if
such a prior information about p is not available we deal with a nonparametric
problem. In nonparametric estimation it is usually assumed that p belongs to
some â€œmassiveâ€ class P of densities. For example, P can be the set of all the
continuous probability densities on R or the set of all the Lipschitz continuous
probability densities on R. Classes of such type will be called nonparametric
classes of functions.
2. Nonparametric regression
Assume that we have n independent pairs of random variables (X1, Y1), . . . ,
(Xn, Yn) such that
Yi = f(Xi) + Î¾i,
Xi âˆˆ[0, 1],
(1.1)
where the random variables Î¾i satisfy E(Î¾i) = 0 for all i and where the func-
tion f from [0, 1] to R (called the regression function) is unknown. The
problem of nonparametric regression is to estimate f given a priori that
this function belongs to a nonparametric class of functions F. For exam-
ple, F can be the set of all the continuous functions on [0, 1] or the set of
A. B. Tsybakov, Introduction to Nonparametric Estimation,


<!-- Page 12 -->

1 Nonparametric estimators
all the convex functions, etc. An estimator of f is a function x â†’fn(x) =
fn(x, X) deï¬ned on [0, 1] and measurable with respect to the observation
X = (X1, . . . , Xn, Y1, . . . , Yn). In what follows, we will mainly focus on the
particular case Xi = i/n.
3. Gaussian white noise model
This is an idealized model that provides an approximation to the nonpara-
metric regression (1.1). Consider the following stochastic diï¬€erential equation:
dY (t) = f(t)dt +
âˆšn dW(t),
t âˆˆ[0, 1],
where W is a standard Wiener process on [0, 1], the function f is an unknown
function on [0, 1], and n is an integer. We assume that a sample path X =
{Y (t), 0 â‰¤t â‰¤1} of the process Y is observed. The statistical problem is to
estimate the unknown function f. In the nonparametric case it is only known
a priori that f âˆˆF where F is a given nonparametric class of functions.
An estimator of f is a function x â†’fn(x) = fn(x, X) deï¬ned on [0, 1] and
measurable with respect to the observation X.
In either of the three above cases, we are interested in the asymptotic
behavior of estimators as n â†’âˆ.
1.2 Kernel density estimators
We start with the ï¬rst of the three problems described in Section 1.1. Let
X1, . . . , Xn be independent identically distributed (i.i.d.) random variables
that have a probability density p with respect to the Lebesgue measure on R.
The corresponding distribution function is F(x) =
 x
âˆ’âˆp(t)dt. Consider the
empirical distribution function
Fn(x) = 1
n
n

i=1
I(Xi â‰¤x),
where I(Â·) denotes the indicator function. By the strong law of large numbers,
we have
Fn(x) â†’F(x),
âˆ€x âˆˆR,
almost surely as n â†’âˆ. Therefore, Fn(x) is a consistent estimator of F(x)
for every x âˆˆR. How can we estimate the density p? One of the ï¬rst intuitive
solutions is based on the following argument. For suï¬ƒciently small h > 0 we
can write an approximation
p(x) â‰ˆF(x + h) âˆ’F(x âˆ’h)
2h
.


<!-- Page 13 -->

1.2 Kernel density estimators
Replacing F by the estimate Fn we deï¬ne
Ë†pR
n (x) = Fn(x + h) âˆ’Fn(x âˆ’h)
2h
.
The function Ë†pR
n is an estimator of p called the Rosenblatt estimator. We can
rewrite it in the form:
Ë†pR
n (x) =
2nh
n

i=1
I(x âˆ’h < Xi â‰¤x + h) = 1
nh
n

i=1
K0
Xi âˆ’x
h

,
where K0(u) = 1
2 I(âˆ’1 < u â‰¤1). A simple generalization of the Rosenblatt
estimator is given by
Ë†pn(x) = 1
nh
n

i=1
K
Xi âˆ’x
h

,
(1.2)
where K : R â†’R is an integrable function satisfying

K(u)du = 1. Such a
function K is called a kernel and the parameter h is called a bandwidth of the
estimator (1.2). The function x â†’Ë†pn(x) is called the kernel density estimator
or the Parzenâ€“Rosenblatt estimator.
In the asymptotic framework, as n â†’âˆ, we will consider a bandwidth h
that depends on n, denoting it by hn, and we will suppose that the sequence
(hn)nâ‰¥1 tends to 0 as n â†’âˆ. The notation h without index n will also be
used for brevity whenever this causes no ambiguity.
Some classical examples of kernels are the following:
K(u) = 1
2 I(|u| â‰¤1) (the rectangular kernel),
K(u) = (1 âˆ’|u|)I(|u| â‰¤1) (the triangular kernel),
K(u) = 3
4 (1 âˆ’u2)I(|u| â‰¤1) (the parabolic kernel,
or the Epanechnikov kernel),
K(u) = 15
16 (1 âˆ’u2)2I(|u| â‰¤1) (the biweight kernel),
K(u) =
âˆš
2Ï€ exp(âˆ’u2/2) (the Gaussian kernel),
K(u) = 1
2 exp(âˆ’|u|/
âˆš
2) sin(|u|/
âˆš
2 + Ï€/4) (the Silverman kernel).
Note that if the kernel K takes only nonnegative values and if X1, . . . , Xn are
ï¬xed, then the function x â†’Ë†pn(x) is a probability density.
The Parzenâ€“Rosenblatt estimator can be generalized to the multidimen-
sional case. For example, we can deï¬ne a kernel density estimator in two di-
mensions as follows. Suppose that we observe n pairs of random variables
(X1, Y1), . . . , (Xn, Yn) such that (Xi, Yi) are i.i.d. with a density p(x, y) in R2.
A kernel estimator of p(x, y) is then given by the formula


<!-- Page 14 -->

1 Nonparametric estimators
Ë†pn(x, y) =
nh2
n

i=1
K
Xi âˆ’x
h

K
Yi âˆ’y
h

(1.3)
where K : R â†’R is a kernel deï¬ned as above and h > 0 is a bandwidth.
1.2.1 Mean squared error of kernel estimators
A basic measure of the accuracy of estimator Ë†pn is its mean squared risk (or
mean squared error) at an arbitrary ï¬xed point x0 âˆˆR:
MSE = MSE(x0)
â–³= Ep

(Ë†pn(x0) âˆ’p(x0))2
.
Here, MSE stands for â€œmean squared errorâ€ and Ep denotes the expectation
with respect to the distribution of (X1, . . . , Xn):
Ep

(Ë†pn(x0) âˆ’p(x0))2 â–³=

. . .

(Ë†pn(x0, x1, . . . , xn) âˆ’p(x0))2
n
	
i=1
[p(xi)dxi] .
We have
MSE = b2(x0) + Ïƒ2(x0)
(1.4)
where
b(x0) = Ep[Ë†pn(x0)] âˆ’p(x0)
and
Ïƒ2(x0) = Ep

Ë†pn(x0) âˆ’Ep[Ë†pn(x0)]
2
.
Deï¬nition 1.1 The quantities b(x0) and Ïƒ2(x0) are called the bias and the
variance of the estimator Ë†pn at a point x0, respectively.
To evaluate the mean squared risk of Ë†pn we will analyze separately its variance
and bias.
Variance of the estimator Ë†pn
Proposition 1.1 Suppose that the density p satisï¬es p(x) â‰¤pmax < âˆfor
all x âˆˆR. Let K : R â†’R be a function such that

K2(u)du < âˆ.
(1.5)
Then for any x0 âˆˆR, h > 0, and n â‰¥1 we have
Ïƒ2(x0) â‰¤C1
nh
where C1 = pmax

K2(u)du.


<!-- Page 15 -->

1.2 Kernel density estimators
Proof. Put
Î·i(x0) = K
Xi âˆ’x0
h

âˆ’Ep

K
Xi âˆ’x0
h

.
The random variables Î·i(x0), i = 1, . . . , n, are i.i.d. with zero mean and vari-
ance
Ep

Î·2
i (x0)

â‰¤Ep

K2
Xi âˆ’x0
h

=

K2
z âˆ’x0
h

p(z)dz â‰¤pmaxh

K2(u)du.
Then
Ïƒ2(x0) = Ep
â¡
â£

nh
n

i=1
Î·i(x0)
2â¤
â¦=
nh2 Ep

Î·2
1(x0)

â‰¤C1
nh .
(1.6)
We conclude that if the bandwidth h = hn is such that nh â†’âˆas n â†’âˆ,
then the variance Ïƒ2(x0) goes to 0 as n â†’âˆ.
Bias of the estimator Ë†pn
The bias of the kernel density estimator has the form
b(x0) = Ep[Ë†pn(x0)] âˆ’p(x0) = 1
h

K
z âˆ’x0
h

p(z)dz âˆ’p(x0).
We now analyze the behavior of b(x0) as a function of h under some regularity
conditions on the density p and on the kernel K.
In what follows âŒŠÎ²âŒ‹will denote the greatest integer strictly less than the
real number Î².
Deï¬nition 1.2 Let T be an interval in R and let Î² and L be two positive
numbers. The HÂ¨older class Î£(Î², L) on T is deï¬ned as the set of â„“= âŒŠÎ²âŒ‹
times diï¬€erentiable functions f : T â†’R whose derivative f (â„“) satisï¬es
|f (â„“)(x) âˆ’f (â„“)(xâ€²)| â‰¤L|x âˆ’xâ€²|Î²âˆ’â„“,
âˆ€x, xâ€² âˆˆT.
Deï¬nition 1.3 Let â„“â‰¥1 be an integer. We say that K : R â†’R is a kernel
of order â„“if the functions u â†’ujK(u), j = 0, 1, . . . , â„“, are integrable and
satisfy

K(u)du = 1,

ujK(u)du = 0,
j = 1, . . . , â„“.


<!-- Page 16 -->

1 Nonparametric estimators
Some examples of kernels of order â„“will be given in Section 1.2.2. It is
important to note that another deï¬nition of an order â„“kernel is often used
in the literature: a kernel K is said to be of order â„“+ 1 (with integer â„“â‰¥1)
if Deï¬nition 1.3 holds and

uâ„“+1K(u)du Ì¸= 0. Deï¬nition 1.3 is less restric-
tive and seems to be more natural, since there is no need to assume that

uâ„“+1K(u)du Ì¸= 0 for noninteger Î². For example, Proposition 1.2 given be-
low still holds if

uâ„“+1K(u)du = 0 and even if this integral does not exist.
Suppose now that p belongs to the class of densities P = P(Î², L) deï¬ned
as follows:
P(Î², L) =

p
 p â‰¥0,

p(x)dx = 1, and p âˆˆÎ£(Î², L) on R

and assume that K is a kernel of order â„“. Then the following result holds.
Proposition 1.2 Assume that p âˆˆP(Î², L) and let K be a kernel of order â„“=
âŒŠÎ²âŒ‹satisfying

|u|Î²|K(u)|du < âˆ.
Then for all x0 âˆˆR, h > 0 and n â‰¥1 we have
|b(x0)| â‰¤C2hÎ²
where
C2 = L
â„“!

|u|Î²|K(u)|du.
Proof. We have
b(x0) = 1
h

K
z âˆ’x0
h

p(z)dz âˆ’p(x0)
=

K(u)

p(x0 + uh) âˆ’p(x0)

du.
Next,
p(x0 + uh) = p(x0) + pâ€²(x0)uh + Â· Â· Â· + (uh)â„“
â„“!
p(â„“)(x0 + Ï„uh),
(1.7)
where 0 â‰¤Ï„ â‰¤1. Since K has order â„“= âŒŠÎ²âŒ‹, we obtain
b(x0) =

K(u)(uh)â„“
â„“!
p(â„“)(x0 + Ï„uh)du
=

K(u)(uh)â„“
â„“!
(p(â„“)(x0 + Ï„uh) âˆ’p(â„“)(x0))du
and


<!-- Page 17 -->

1.2 Kernel density estimators
|b(x0)| â‰¤

|K(u)||uh|â„“
â„“!
p(â„“)(x0 + Ï„uh) âˆ’p(â„“)(x0)
du
â‰¤L

|K(u)||uh|â„“
â„“!
|Ï„uh|Î²âˆ’â„“du â‰¤C2hÎ².
Upper bound on the mean squared risk
From Propositions 1.1 and 1.2, we see that the upper bounds on the bias and
variance behave in opposite ways as the bandwidth h varies. The variance de-
creases as h grows, whereas the bound on the bias increases (cf. Figure 1.1).
The choice of a small h corresponding to a large variance is called an un-
Bias/Variance tradeoff
hâˆ—
n
Bias squared
Variance
Figure 1.1. Squared bias, variance, and mean squared error (solid line)
as functions of h.
dersmoothing. Alternatively, with a large h the bias cannot be reasonably
controlled, which leads to oversmoothing. An optimal value of h that balances
bias and variance is located between these two extremes. Figure 1.2 shows
typical plots of the corresponding density estimators. To get an insight into
the optimal choice of h, we can minimize in h the upper bound on the MSE
obtained from the above results.
If p and K satisfy the assumptions of Propositions 1.1 and 1.2, we obtain
MSE â‰¤C2
2h2Î² + C1
nh .
(1.8)


<!-- Page 18 -->

1 Nonparametric estimators
Undersmoothing
Oversmoothing
Correct smoothing
Figure 1.2. Undersmoothing, oversmoothing, and correct smoothing.
The circles indicate the sample points Xi.
The minimum with respect to h of the right hand side of (1.8) is attained
at
hâˆ—
n =
 C1
2Î²C2

2Î²+1
nâˆ’
2Î²+1 .
Therefore, the choice h = hâˆ—
n gives
MSE(x0) = O

nâˆ’
2Î²
2Î²+1

,
n â†’âˆ,
uniformly in x0. We have the following result.


<!-- Page 19 -->

1.2 Kernel density estimators
Theorem 1.1 Assume that condition (1.5) holds and the assumptions of Pro-
position 1.2 are satisï¬ed. Fix Î± > 0 and take h = Î±nâˆ’
2Î²+1 . Then for n â‰¥1
the kernel estimator Ë†pn satisï¬es
sup
x0âˆˆR
sup
pâˆˆP(Î²,L)
Ep[(Ë†pn(x0) âˆ’p(x0))2] â‰¤Cnâˆ’
2Î²
2Î²+1 ,
where C > 0 is a constant depending only on Î², L, Î± and on the kernel K.
Proof. We apply (1.8) as shown above. To justify the application of Proposi-
tion 1.1, it remains to prove that there exists a constant pmax < âˆsatisfying
sup
xâˆˆR
sup
pâˆˆP(Î²,L)
p(x) â‰¤pmax.
(1.9)
To show (1.9), consider Kâˆ—which is a bounded kernel of order â„“, not neces-
sarily equal to K. Applying Proposition 1.2 with h = 1 we get that, for any
x0 âˆˆR and any p âˆˆP(Î², L),


Kâˆ—(z âˆ’x0)p(z)dz âˆ’p(x0)
 â‰¤Câˆ—
â–³= L
â„“!

|u|Î²|Kâˆ—(u)|du.
Therefore, for any x âˆˆR and any p âˆˆP(Î², L),
p(x) â‰¤Câˆ—
2 +

|Kâˆ—(z âˆ’x)|p(z)dz â‰¤Câˆ—
2 + Kâˆ—
max,
where Kâˆ—
max = supuâˆˆR |Kâˆ—(u)|. Thus, we get (1.9) with pmax = Câˆ—
2 + Kâˆ—
max.
Under the assumptions of Theorem 1.1, the rate of convergence of the es-
timator Ë†pn(x0) is Ïˆn = nâˆ’
Î²
2Î²+1 , which means that for a ï¬nite constant C and
for all n â‰¥1 we have
sup
pâˆˆP(Î²,L)
Ep

(Ë†pn(x0) âˆ’p(x0))2
â‰¤CÏˆ2
n.
Now the following two questions arise. Can we improve the rate Ïˆn by using
other density estimators? What is the best possible rate of convergence? To
answer these questions it is useful to consider the minimax risk Râˆ—
n associated
to the class P(Î², L):
Râˆ—
n(P(Î², L))
â–³= inf
Tn
sup
pâˆˆP(Î²,L)
Ep

(Tn(x0) âˆ’p(x0))2
,
where the inï¬mum is over all estimators. One can prove a lower bound on
the minimax risk of the form Râˆ—
n(P(Î², L)) â‰¥Câ€²Ïˆ2
n = Câ€²nâˆ’
2Î²
2Î²+1 with some
constant Câ€² > 0 (cf. Chapter 2, Exercise 2.8). This implies that under the
assumptions of Theorem 1.1 the kernel estimator attains the optimal rate
of convergence nâˆ’
Î²
2Î²+1 associated with the class of densities P(Î², L). Exact
deï¬nitions and discussions of the notion of optimal rate of convergence will
be given in Chapter 2.


<!-- Page 20 -->

1 Nonparametric estimators
Positivity constraint
It follows easily from Deï¬nition 1.3 that kernels of order â„“â‰¥2 must take
negative values on a set of positive Lebesgue measure. The estimators Ë†pn
based on such kernels can also take negative values. This property is sometimes
emphasized as a drawback of estimators with higher order kernels, since the
density p itself is nonnegative. However, this remark is of minor importance
because we can always use the positive part estimator
Ë†p+
n (x)
â–³= max{0, Ë†pn(x)}
whose risk is smaller than or equal to the risk of Ë†pn:
Ep

(Ë†p+
n (x0) âˆ’p(x0))2
â‰¤Ep

(Ë†pn(x0) âˆ’p(x0))2
,
âˆ€x0 âˆˆR.
(1.10)
In particular, Theorem 1.1 remains valid if we replace there Ë†pn by Ë†p+
n . Thus,
the estimator Ë†p+
n is nonnegative and attains fast convergence rates associated
with higher order kernels.

---
*[End of extracted section containing Propositions 1.1 and 1.2]*
